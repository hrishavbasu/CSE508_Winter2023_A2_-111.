{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3QL7-AEdP_u",
        "outputId": "51d829d1-dce2-4a1f-81ae-369a0d80987f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfJ2Lo_6jBty"
      },
      "outputs": [],
      "source": [
        "database = \"/content/drive/MyDrive/CSE508_Winter2023_Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTwOSPgv3IYl",
        "outputId": "c8642cea-9fd8-4af7-f1f5-bfd055663c1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (4.6.3)\n"
          ]
        }
      ],
      "source": [
        "pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5yh38-M3ltR"
      },
      "outputs": [],
      "source": [
        "\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyMy7DDsnOuf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "c8ba7c5d-f188-4a7b-ee5d-416a9ceebc8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a simple method of matric structural analysis, part\n",
            "iv, non-linear problems .\n",
            "  the method presented in the previous parts is employed to\n",
            "solve various kinds of nonlinear problems, such as problems concerning\n",
            "large deflections or buckling, or thermal creep, or inelastic\n",
            "stress redistribution involving thermal gradients, or design\n",
            ".  the procedure used in each case is one of direct iteration-i.\n",
            "e., after one assumes a starting point all subsequent cycles are\n",
            "self-generating .  simple numerical examples are worked out .\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-48325dbe8375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# call read text file function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mfile_handling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-48325dbe8375>\u001b[0m in \u001b[0;36mfile_handling\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m# appending elemnet to the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Change the directory\n",
        "os.chdir(database)\n",
        "  \n",
        "def file_handling(file_path):\n",
        "  with open(file_path) as fp:\n",
        "    soup = BeautifulSoup(fp, 'html.parser')\n",
        "\n",
        "  # storing content in a list format\n",
        "  content=[]\n",
        "\n",
        "  # reading all files before editing\n",
        "  f = open(file_path, \"r\")\n",
        "  print(f.read())\n",
        "  # print(soup.title.text.strip() , soup.find('text').text)\n",
        "\n",
        "  # appending elemnet to the list \n",
        "  content.append(soup.title.text.strip())\n",
        "  content.append(soup.find('text').text.strip(\"\"))\n",
        "\n",
        "  # writing in same file the result \n",
        "  f = open(file_path, \"w\")\n",
        "  for x in content:\n",
        "    f.write(x)\n",
        "  f.close()\n",
        "  content.clear()\n",
        "\n",
        "  # reading all files after editing\n",
        "  f = open(file_path, \"r\")\n",
        "  print(f.read())\n",
        "\n",
        "\n",
        "# iterate through all file\n",
        "for file in os.listdir():\n",
        "    # Check whether file is in text format or not\n",
        "        file_path = f\"{database}/{file}\"\n",
        "  \n",
        "        # call read text file function\n",
        "        file_handling(file_path) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1**"
      ],
      "metadata": {
        "id": "Ab1kwnVu9xT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os import popen\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import string\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from typing import List\n",
        "import os\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "id": "-xMKhg9_bv-M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58681e16-e49e-421f-9be8-8d54debe6f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TF-IDF Matrix**"
      ],
      "metadata": {
        "id": "aojK8N0x4zo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize(doc):\n",
        "    tokens = word_tokenize(doc.lower())\n",
        "    tokens = [token for token in tokens if token not in stopwords.words('english') + list(string.punctuation)]\n",
        "    tokens = [PorterStemmer().stem(token) for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def calculate_tf(tokens, weight_scheme):\n",
        "    tf = {}\n",
        "    if weight_scheme == 'binary':\n",
        "        tf = {token: 1 for token in tokens}\n",
        "    elif weight_scheme == 'raw':\n",
        "        tf = Counter(tokens)\n",
        "    elif weight_scheme == 'term_frequency':\n",
        "        tf = {token: count/len(tokens) for token, count in Counter(tokens).items()}\n",
        "    elif weight_scheme == 'log_norm':\n",
        "        max_count = max(Counter(tokens).values())\n",
        "        tf = {token: 1 + math.log(count) / math.log(max_count) if max_count > 1 else 0 for token, count in Counter(tokens).items()}\n",
        "    elif weight_scheme == 'double_norm':\n",
        "        max_count = max(Counter(tokens).values())\n",
        "        tf = {token: 0.5 + 0.5 * count / max_count for token, count in Counter(tokens).items()}\n",
        "    else:\n",
        "        raise ValueError('Invalid weight_scheme: {}'.format(weight_scheme))\n",
        "    return tf\n",
        "\n",
        "\n",
        "def calculate_idf(tokenized_docs, term):\n",
        "    num_docs_with_term = len([True for doc in tokenized_docs if term in doc])\n",
        "    return math.log(len(tokenized_docs) / num_docs_with_term)\n",
        "\n",
        "\n",
        "def calculate_tfidf(tokenized_doc, tf, idf):\n",
        "    return {term: tf[term] * idf[term] for term in tokenized_doc}\n",
        "\n",
        "\n",
        "def create_tfidf_matrix(docs, weight_scheme):\n",
        "    tokenized_docs = [tokenize(doc) for doc in docs]\n",
        "    vocab = list(set([token for doc in tokenized_docs for token in doc]))\n",
        "    idf = {term: calculate_idf(tokenized_docs, term) for term in vocab}\n",
        "    tfidf_matrix = []\n",
        "    for tokenized_doc in tokenized_docs:\n",
        "        tf = calculate_tf(tokenized_doc, weight_scheme)\n",
        "        tfidf = calculate_tfidf(tokenized_doc, tf, idf)\n",
        "        tfidf_matrix.append(tfidf)\n",
        "    return tfidf_matrix\n",
        "\n",
        "\n",
        "docs = []\n",
        "data_dir = database\n",
        "for file_name in os.listdir(data_dir):\n",
        "    file_path = os.path.join(data_dir, file_name)\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        docs.append(f.read())\n",
        "\n",
        "# Get user query and tokenize it\n",
        "query = input('Enter your query: ')\n",
        "query_tokens = tokenize(query)\n",
        "\n",
        "# Calculate TF-IDF scores for query using all five weight schemes\n",
        "weight_schemes = ['binary', 'raw', 'term_frequency', 'log_norm', 'double_norm']\n",
        "for weight_scheme in weight_schemes:\n",
        "    print('Weight scheme: {}'.format(weight_scheme))\n",
        "    tfidf_matrix = create_tfidf_matrix(docs, weight_scheme)\n",
        "    query_tf = calculate_tf(query_tokens, weight_scheme)\n",
        "    query_tfidf = calculate_tfidf(query_tokens, query_tf, {term: 1 for term in query_tokens})\n",
        "    for i, doc_tfidf in enumerate(tfidf_matrix):\n",
        "        doc_score = sum([score for term, score in query_tfidf.items() if term in doc_tfidf])\n",
        "        print('Doc {}: {}'.format(i+1, doc_score))"
      ],
      "metadata": {
        "id": "ugTLxcnYxyIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Jaccard Coefficient**"
      ],
      "metadata": {
        "id": "xVExCI9z4fj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_coefficient(query: str, document: str) -> float:\n",
        "    query_tokens = set(query.lower().split())\n",
        "    document_tokens = set(document.lower().split())\n",
        "    intersection_size = len(query_tokens.intersection(document_tokens))\n",
        "    union_size = len(query_tokens.union(document_tokens))\n",
        "    return intersection_size / union_size\n",
        "\n",
        "def rank_documents_by_jaccard_coefficient(query: str, documents: List[str], filenames: List[str]) -> List[str]:\n",
        "    scores = [(filename, jaccard_coefficient(query, document)) for filename, document in zip(filenames, documents)]\n",
        "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "    return [filename for filename, _ in sorted_scores][:10]\n",
        "\n",
        "filenames = os.listdir(database)\n",
        "documents = [open(os.path.join(database, filename), \"r\").read() for filename in filenames]\n",
        "\n",
        "query = input('Enter your query: ')\n",
        "top_documents = rank_documents_by_jaccard_coefficient(query, documents, filenames)\n",
        "print(top_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjBeoZ0_4wh4",
        "outputId": "0cf3267b-d6fb-4658-d2d2-c2231bcdc57e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: procedure analysis\n",
            "['cranfield1071', 'cranfield0832', 'cranfield1358', 'cranfield1054', 'cranfield0619', 'cranfield1276', 'cranfield1048', 'cranfield0551', 'cranfield1357', 'cranfield0326']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q2**"
      ],
      "metadata": {
        "id": "B7E_XZZ69rnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/BBC News Sample Solution.csv')\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop('ArticleId', axis=1, inplace=True)\n",
        "\n",
        "# Clean text by removing punctuation and converting to lowercase\n",
        "df['Text'] = df['Text'].str.replace('[^\\w\\s]', '').str.lower()\n",
        "\n",
        "# Tokenize text by splitting it into words\n",
        "df['Text'] = df['Text'].apply(nltk.word_tokenize)\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['Text'] = df['Text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n",
        "# Perform stemming or lemmatization\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['Text'] = df['Text'].apply(lambda x: [stemmer.stem(word) for word in x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U7IEv9_mENcD",
        "outputId": "2113dbaa-9926-40aa-ce14-cc5aaf8ea601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-45-5d30354c6dbb>\", line 8, in <module>\n",
            "    df = pd.read_csv('/content/BBC News Sample Solution.csv')\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\", line 678, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\", line 575, in _read\n",
            "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\", line 932, in __init__\n",
            "    self._engine = self._make_engine(f, self.engine)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\", line 1216, in _make_engine\n",
            "    self.handles = get_handle(  # type: ignore[call-overload]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\", line 658, in get_handle\n",
            "    if _is_binary_mode(path_or_buf, mode) and \"b\" not in mode:\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\", line 1125, in _is_binary_mode\n",
            "    return isinstance(handle, _get_binary_io_classes()) or \"b\" in getattr(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\", line 1140, in _get_binary_io_classes\n",
            "    zstd = import_optional_dependency(\"zstandard\", errors=\"ignore\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pandas/compat/_optional.py\", line 138, in import_optional_dependency\n",
            "    module = importlib.import_module(name)\n",
            "  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 982, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 925, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1423, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1392, in _get_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1356, in _path_importer_cache\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1543, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1501, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 738, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 722, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.9/posixpath.py\", line 380, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}